# Theoretical Foundations of Risk Management
# PURPOSE: Principles that constrain what risk management CAN and CANNOT achieve
# LOAD: workflow.md, step-00-ground.md

---
foundations:
  - id: NAT
    name: "Normal Accident Theory"
    author: "Perrow, 1984"
    one_liner: "Complex + tightly coupled = accidents inevitable"
    description: |
      In systems that are simultaneously COMPLEX (many non-linear interactions)
      and TIGHTLY COUPLED (little slack, fast propagation), accidents are
      INEVITABLE — not a matter of IF but WHEN.
    implication: |
      For such systems, the question shifts from "how to prevent all failures"
      to "how to survive inevitable failures."
    applied_in: ["#003", "#305", "#404"]
    check_question: "Is the system in the Normal Accidents zone (complexity ≥4, coupling ≥4)?"

  - id: ERGODICITY
    name: "Non-Ergodicity"
    author: "Peters, 2019"
    one_liner: "Can't average over what you only experience once"
    description: |
      Ensemble average ≠ time average. A 5% chance of total loss is acceptable
      for a casino playing 10,000 games simultaneously. It is CATASTROPHIC for
      you playing sequentially — because you cannot recover from ruin to play again.
    implication: |
      Most risk scores (P × I) implicitly assume ergodicity. For irreversible
      losses, this assumption is FATALLY WRONG. A risk with P=0.05 and
      I="game over" is NOT equivalent to P=0.50 and I="minor inconvenience"
      — even if expected value is similar.
    applied_in: ["#206", "#401", "#408"]
    check_question: "If this materializes, can we continue, or is it game over?"

  - id: FAT_TAILS
    name: "Fat Tails / Power Laws"
    author: "Mandelbrot, Taleb"
    one_liner: "Extremes dominate, means mislead"
    description: |
      Many risks do NOT follow normal distributions. Impact is often power-law
      distributed: "Impact: 5" is not 5× worse than "Impact: 1" — it can be
      100× or 1000× worse.
    implication: |
      The mean is dominated by rare extreme events. Variance may be infinite.
      Historical data systematically underestimates tail risk.
    applied_in: ["#201", "#203", "#402"]
    check_question: "Could the actual impact be orders of magnitude worse than scored?"

  - id: SWISS_CHEESE
    name: "Swiss Cheese Model"
    author: "Reason, 1990"
    one_liner: "Aligned holes in layers = failure"
    description: |
      Accidents occur when holes in multiple defense layers momentarily align.
      Each layer has holes (weaknesses). Safety depends on layers being
      INDEPENDENT — if their holes are correlated, multiple layers offer
      the protection of one.
    implication: |
      Defense in Depth is only as strong as the INDEPENDENCE between layers.
      Same person maintaining multiple layers, same vendor, same infrastructure
      = correlated holes = false redundancy.
    applied_in: ["#303", "#403"]
    check_question: "Are defense layer holes correlated (same person, vendor, infra)?"

  - id: COBRA_EFFECT
    name: "Cobra Effect / Braess Paradox"
    author: "Historical"
    one_liner: "Interventions can backfire"
    description: |
      Interventions can produce outcomes OPPOSITE to intentions. British India
      offered bounty for dead cobras → people bred cobras → bounty cancelled
      → bred cobras released → more cobras than before. Adding a road to a
      network can increase total travel time (Braess).
    implication: |
      Every mitigation must be checked for perverse second-order effects.
      The cure can be worse than the disease.
    applied_in: ["#307", "#407"]
    check_question: "Does this mitigation create new risks worse than the original?"

  - id: GOODHART
    name: "Goodhart's Law"
    author: "Goodhart, 1975"
    one_liner: "Measured target → gamed target"
    description: |
      When a measure becomes a target, it ceases to be a good measure. A risk
      dashboard showing zero alerts may mean: (a) no risks, or (b) team stopped
      reporting risks, or (c) thresholds set too high.
    implication: |
      Risk metrics must be audited for gaming/decay. Rotate metrics periodically.
      Check incentive alignment.
    applied_in: ["#501", "#606"]
    check_question: "Could this metric be gamed? Are incentives aligned with honest reporting?"

  - id: KNIGHT
    name: "Knight's Distinction"
    author: "Knight, 1921"
    one_liner: "Risk ≠ Uncertainty ≠ Ambiguity"
    description: |
      Three fundamentally different epistemic states:
      - RISK: Known probability distribution → Calculate, hedge, insure
      - UNCERTAINTY: Unknown probability distribution → Scenario plan, build optionality
      - AMBIGUITY: Question itself unclear → Clarify, decompose, define
    implication: |
      Applying the wrong strategy for the type is itself a risk. Assigning
      P=3 to Knight-Uncertainty is faux precision.
    applied_in: ["#002", "#201", "#402"]
    check_question: "Is this risk (known distribution), uncertainty (unknown), or ambiguity (unclear)?"

  - id: SURVIVORSHIP
    name: "Survivorship Bias"
    author: "Wald, WW2"
    one_liner: "We only learn from visible failures"
    description: |
      We only learn from failures that were visible. For every well-known
      post-mortem, there are N companies that failed identically and
      disappeared without documenting anything.
    implication: |
      Historical pattern matching systematically underestimates risk because
      the evidence set is biased toward survivors.
    applied_in: ["#106", "#204"]
    check_question: "How many projects failed this exact way and we never heard about them?"

  - id: LINDY
    name: "Lindy Effect"
    author: "Mandelbrot, Taleb"
    one_liner: "Old = robust, new = fragile"
    description: |
      Things that have survived long will likely survive longer. Conversely:
      NEW components, processes, relationships, and technologies have HIGHER
      base-rate risk of failure than established ones.
    implication: |
      Age is evidence of robustness — novelty is a risk factor. New dependencies
      should be treated with more caution than established ones.
    applied_in: ["#104", "#111"]
    check_question: "How new is this component/dependency? New = higher base-rate failure risk."

---
# Integration with Deep-Verify impossibility theorems
theorem_integration:
  - theorem: "FLP"
    statement: "Async consensus impossible with failures"
    risk_implication: "Any claim of reliable distributed consensus under partition = hidden risk"

  - theorem: "CAP"
    statement: "Pick 2 of 3 (Consistency, Availability, Partition tolerance)"
    risk_implication: "System claiming C+A+P = unidentified trade-off hiding in design"

  - theorem: "Halting"
    statement: "Guaranteed termination of arbitrary computation impossible"
    risk_implication: "Claims of guaranteed termination for unbounded input = impossible"

  - theorem: "Arrow"
    statement: "No voting system satisfies all fairness criteria simultaneously"
    risk_implication: "Consensus mechanisms claiming perfect fairness = hidden trade-off"

  - theorem: "No-Free-Lunch"
    statement: "No optimizer is universally best"
    risk_implication: "Claims of universally optimal solution = domain-specific assumption hidden"

---
# Quick reference for workflow
quick_reference:
  - principle: "Normal Accidents"
    when_to_apply: "System characterization (GROUND)"
    key_question: "Is this complex + coupled?"

  - principle: "Non-Ergodicity"
    when_to_apply: "High-impact risk scoring (QUANTIFY)"
    key_question: "Can we survive this?"

  - principle: "Fat Tails"
    when_to_apply: "Impact estimation (QUANTIFY)"
    key_question: "Could this be 100× worse?"

  - principle: "Swiss Cheese"
    when_to_apply: "Defense design (MITIGATE)"
    key_question: "Are layers independent?"

  - principle: "Cobra Effect"
    when_to_apply: "Mitigation design (MITIGATE)"
    key_question: "Does this create worse risks?"

  - principle: "Goodhart's Law"
    when_to_apply: "Monitoring design (MONITOR)"
    key_question: "Can this be gamed?"

  - principle: "Knight's Distinction"
    when_to_apply: "Uncertainty classification (GROUND)"
    key_question: "Risk, uncertainty, or ambiguity?"

  - principle: "Survivorship Bias"
    when_to_apply: "Historical analysis (IDENTIFY)"
    key_question: "What failures are invisible?"

  - principle: "Lindy Effect"
    when_to_apply: "Dependency analysis (IDENTIFY)"
    key_question: "How new is this?"
