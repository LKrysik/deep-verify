# Data Engineering Risk Pattern Library
# Patterns specific to data pipelines, warehouses, and analytics

library:
  id: "data-engineering"
  name: "Data Engineering Risk Patterns"
  version: "1.0.0"
  description: "Failure patterns observed in data pipelines, ETL/ELT, data warehouses, and analytics systems"

patterns:
  # ============================================================================
  # DATA QUALITY PATTERNS
  # ============================================================================

  - id: "DE-001"
    name: "Silent Data Corruption"
    description: |
      Data quality degrades without detection. Bad data flows through the
      pipeline and pollutes downstream systems before anyone notices.

    signature:
      - "No data quality checks between stages"
      - "Quality measured at destination only (if at all)"
      - "Source system changes not detected"
      - "'Trust the data' culture"

    anti_patterns:
      - "No schema validation at ingestion"
      - "No row count reconciliation"
      - "No statistical anomaly detection"
      - "Quality checks only in batch, not real-time"

    indicators:
      - "User-reported data discrepancies"
      - "Late discovery of data issues"
      - "Root cause analysis traces back days/weeks"

    historical_examples:
      - "Financial reporting: Bad upstream data caused restatements"
      - "ML pipelines: Training on corrupted data silently"

    mitigation_hints:
      - "Data quality gates at each stage"
      - "Great Expectations or similar frameworks"
      - "Row count and checksum reconciliation"

  - id: "DE-002"
    name: "Schema Drift Catastrophe"
    description: |
      Source schema changes without warning, breaking downstream consumers.
      The contract between producer and consumer is implicit and unmonitored.

    signature:
      - "Source systems can change schema at will"
      - "No schema registry or contract"
      - "Downstream assumes stable schema"
      - "Schema changes discovered by failures"

    anti_patterns:
      - "No schema version tracking"
      - "No backward compatibility requirements"
      - "Hard-coded column positions/names"
      - "No schema diff monitoring"

    indicators:
      - "Pipeline failures after upstream releases"
      - "NULL values where not expected"
      - "Type coercion errors"

    historical_examples:
      - "Common: API changes break data pipelines"
      - "Database migrations breaking ETL jobs"

    mitigation_hints:
      - "Schema registry (Confluent, Glue)"
      - "Schema diff alerting"
      - "Backward-compatible evolution policies"

  - id: "DE-003"
    name: "Null Propagation Blindness"
    description: |
      NULL values enter the pipeline and propagate silently, causing incorrect
      aggregations, joins, or calculations downstream.

    signature:
      - "Source can produce NULL for expected values"
      - "NULL handling not explicit in transformations"
      - "Aggregations include NULL handling (avg, count, etc.)"
      - "Joins with nullable keys"

    anti_patterns:
      - "Implicit NULL handling (language defaults)"
      - "No NOT NULL constraints or validation"
      - "NULL used to mean multiple things"
      - "No monitoring of NULL rates"

    indicators:
      - "Changing NULL percentages"
      - "Aggregation results don't match expectations"
      - "Missing rows after joins"

    historical_examples:
      - "Common: NULL in join key causes silent data loss"
      - "Average calculations skewed by NULL exclusion"

    mitigation_hints:
      - "Explicit NULL handling in all transformations"
      - "NULL rate monitoring"
      - "NULL-safe joins and aggregations"

  # ============================================================================
  # PIPELINE RELIABILITY PATTERNS
  # ============================================================================

  - id: "DE-004"
    name: "Pipeline Timing Fragility"
    description: |
      Pipeline depends on data arriving at specific times. Late data, clock
      skew, or timezone issues cause incorrect processing.

    signature:
      - "Time-based batch processing"
      - "Assumption that 'daily data arrives by X'"
      - "No late data handling"
      - "Timezone handling inconsistent"

    anti_patterns:
      - "Hard-coded time boundaries"
      - "No late arrival tolerance"
      - "Mixed timezone handling"
      - "Processing triggered by time, not data arrival"

    indicators:
      - "Missing data on DST transitions"
      - "Incomplete batches at cut-off"
      - "Data appearing in wrong day's batch"

    historical_examples:
      - "End-of-month reports missing late transactions"
      - "Timezone bugs around daylight saving"

    mitigation_hints:
      - "Event-time processing (not wall-clock)"
      - "Watermarks and late data handling"
      - "UTC everywhere, convert at display"

  - id: "DE-005"
    name: "Backfill Impossibility"
    description: |
      Historical data cannot be reprocessed because pipeline wasn't designed
      for it. Fixing past errors requires heroic manual effort.

    signature:
      - "Pipeline processes only current data"
      - "No idempotent re-run capability"
      - "State accumulated without checkpoints"
      - "'Fixing historical data' is a project"

    anti_patterns:
      - "Incremental-only processing"
      - "In-place updates without audit trail"
      - "No partition management"
      - "Source data not retained"

    indicators:
      - "Known data quality issues in historical data"
      - "Requests to fix past data denied as impossible"
      - "Manual SQL scripts for corrections"

    historical_examples:
      - "Compliance reporting: Cannot regenerate past reports"
      - "Analytics: Historical metrics frozen despite known errors"

    mitigation_hints:
      - "Idempotent pipeline design"
      - "Partition-based processing"
      - "Source data retention"

  - id: "DE-006"
    name: "DAG Spaghetti Entanglement"
    description: |
      Pipeline dependencies become so complex that changes are risky,
      debugging is hard, and understanding data lineage is impossible.

    signature:
      - "Hundreds of interconnected jobs"
      - "No clear ownership of pipeline segments"
      - "Circular or unclear dependencies"
      - "Changes require archaeology"

    anti_patterns:
      - "Organic pipeline growth without architecture"
      - "No data lineage tracking"
      - "Copy-paste job creation"
      - "No pipeline documentation"

    indicators:
      - "Long time to understand impact of changes"
      - "Frequent unexpected downstream breaks"
      - "No one understands full pipeline"

    historical_examples:
      - "Common in mature data platforms"
      - "Legacy ETL systems"

    mitigation_hints:
      - "Data lineage tools"
      - "Modular pipeline architecture"
      - "Regular pipeline review and cleanup"

  # ============================================================================
  # SCHEMA MANAGEMENT PATTERNS
  # ============================================================================

  - id: "DE-007"
    name: "Slowly Changing Dimension Decay"
    description: |
      Dimension management becomes incorrect over time. Historical analysis
      uses wrong dimension values or loses track of changes.

    signature:
      - "Dimensions that change over time (org structure, geography)"
      - "SCD type not matched to use case"
      - "No effective dating or versioning"
      - "Reports show current state for historical data"

    anti_patterns:
      - "Type 1 SCD when history matters"
      - "Inconsistent SCD handling across dimensions"
      - "No point-in-time query capability"
      - "Dimension changes not tracked"

    indicators:
      - "Historical reports change after dimension updates"
      - "Audit questions about historical accuracy"
      - "Join issues with fact tables"

    historical_examples:
      - "Sales credited to wrong region after reorg"
      - "Customer data showing current state for old orders"

    mitigation_hints:
      - "Choose appropriate SCD type"
      - "Effective dating with as-of queries"
      - "Audit trail for dimension changes"

  - id: "DE-008"
    name: "Semantic Layer Drift"
    description: |
      Business definitions of metrics drift from technical implementation.
      The same metric name means different things to different people.

    signature:
      - "Multiple definitions of 'revenue', 'user', 'active'"
      - "Different teams calculate same metric differently"
      - "No single source of truth for definitions"
      - "Business and technical out of sync"

    anti_patterns:
      - "Metric definitions in spreadsheets or tribal knowledge"
      - "Different SQL for same metric in different reports"
      - "No metric governance"
      - "Calculated fields duplicated across tools"

    indicators:
      - "Numbers don't match between reports"
      - "'Which revenue?' in meetings"
      - "Time spent reconciling metrics"

    historical_examples:
      - "Finance and Marketing have different revenue numbers"
      - "Executive dashboards inconsistent"

    mitigation_hints:
      - "Semantic layer (metrics layer)"
      - "Metric definitions as code"
      - "Single source of truth"

  # ============================================================================
  # PERFORMANCE PATTERNS
  # ============================================================================

  - id: "DE-009"
    name: "Query of Death"
    description: |
      A single expensive query can bring down the entire data platform,
      affecting all users.

    signature:
      - "Shared compute resources"
      - "No query governance or limits"
      - "Users can write arbitrary queries"
      - "No workload isolation"

    anti_patterns:
      - "No query timeout limits"
      - "No resource quotas"
      - "SELECT * on large tables allowed"
      - "No query review for expensive patterns"

    indicators:
      - "Platform slowdowns traced to single query"
      - "User complaints about performance"
      - "Runaway queries in logs"

    historical_examples:
      - "Common: Accidental cartesian join taking down warehouse"
      - "Full table scan blocking production queries"

    mitigation_hints:
      - "Query governors and timeouts"
      - "Workload isolation"
      - "Query cost estimation and approval"

  - id: "DE-010"
    name: "Data Skew Bottleneck"
    description: |
      Uneven data distribution causes some partitions/workers to be
      overwhelmed while others sit idle.

    signature:
      - "Distributed processing (Spark, Presto)"
      - "Non-uniform key distribution"
      - "One stage takes much longer than others"
      - "Adding resources doesn't help"

    anti_patterns:
      - "Group by on low-cardinality keys"
      - "Join on skewed keys"
      - "No salting for hot keys"
      - "Ignoring data distribution"

    indicators:
      - "Task duration variance very high"
      - "One executor doing all the work"
      - "OOM errors on specific partitions"

    historical_examples:
      - "Common: NULL key skew in Spark joins"
      - "Celebrity/popular item skew"

    mitigation_hints:
      - "Salting hot keys"
      - "Broadcast joins for small tables"
      - "Data distribution analysis"

  - id: "DE-011"
    name: "Small File Proliferation"
    description: |
      Many small files accumulate, degrading read performance and
      overwhelming metadata management.

    signature:
      - "Streaming or frequent micro-batch writes"
      - "Many files per partition"
      - "Query performance degrading over time"
      - "Listing operations slow"

    anti_patterns:
      - "No file compaction process"
      - "Streaming writes without optimization"
      - "Too many partitions"
      - "No cleanup of temporary files"

    indicators:
      - "Increasing query times"
      - "Storage metadata operations slow"
      - "File counts in millions"

    historical_examples:
      - "Hive/Spark performance degradation from small files"
      - "Object storage listing timeouts"

    mitigation_hints:
      - "Regular compaction jobs"
      - "Optimize for file size (100MB-1GB)"
      - "Delta Lake / Iceberg auto-compaction"

  # ============================================================================
  # GOVERNANCE PATTERNS
  # ============================================================================

  - id: "DE-012"
    name: "PII Leakage Through Pipeline"
    description: |
      Personally identifiable information flows into systems where it
      shouldn't be, creating compliance and security risks.

    signature:
      - "Data flows through multiple systems"
      - "No classification of sensitive data"
      - "PII handling inconsistent across pipeline"
      - "Downstream systems unaware of PII presence"

    anti_patterns:
      - "No data classification"
      - "No PII detection in pipelines"
      - "Copy data without considering sensitivity"
      - "Dev/test environments with production PII"

    indicators:
      - "PII discovered in unexpected places"
      - "Compliance audit findings"
      - "Access to sensitive data too broad"

    historical_examples:
      - "GDPR violations from untracked PII"
      - "PII in log files, analytics systems"

    mitigation_hints:
      - "Data classification and tagging"
      - "PII detection in pipeline"
      - "Anonymization/pseudonymization"

  - id: "DE-013"
    name: "Lineage Blackhole"
    description: |
      Data enters the platform but its origin and transformations are not
      tracked. When issues arise, root cause analysis is impossible.

    signature:
      - "No data lineage tracking"
      - "Cannot answer 'where did this data come from?'"
      - "Cannot answer 'who/what uses this data?'"
      - "Impact analysis requires manual investigation"

    anti_patterns:
      - "No lineage metadata capture"
      - "Manual documentation of data flows"
      - "Transformations without lineage hooks"
      - "Multiple tools without lineage integration"

    indicators:
      - "Long time to investigate data issues"
      - "Unknown downstream impact of changes"
      - "Orphaned datasets"

    historical_examples:
      - "Common: Cannot trace data quality issue to source"
      - "Deprecation of tables with unknown consumers"

    mitigation_hints:
      - "Automated lineage capture"
      - "OpenLineage standard"
      - "Data catalog with lineage"

  - id: "DE-014"
    name: "Data Swamp Formation"
    description: |
      Data lake becomes a data swamp: undocumented, ungoverned, unreliable
      data that no one trusts or uses effectively.

    signature:
      - "Easy to add data, hard to find/trust data"
      - "No catalog or documentation"
      - "Duplicate datasets with unclear differences"
      - "Users maintain personal copies"

    anti_patterns:
      - "Ingest everything, figure out later"
      - "No data stewardship"
      - "No quality requirements for publishing"
      - "No lifecycle management"

    indicators:
      - "Low data discovery success rate"
      - "Shadow data systems"
      - "Storage costs growing faster than value"

    historical_examples:
      - "Many failed data lake initiatives"
      - "'Data lake became data graveyard'"

    mitigation_hints:
      - "Data catalog and discovery"
      - "Data stewardship model"
      - "Quality gates for publishing"
