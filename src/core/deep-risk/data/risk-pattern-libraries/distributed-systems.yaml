# Distributed Systems Risk Pattern Library
# Patterns specific to distributed and networked systems

library:
  id: "distributed-systems"
  name: "Distributed Systems Risk Patterns"
  version: "1.0.0"
  description: "Failure patterns observed in distributed systems, microservices, and networked architectures"

patterns:
  # ============================================================================
  # CONSENSUS PATTERNS
  # ============================================================================

  - id: "DS-001"
    name: "Split Brain Catastrophe"
    description: |
      Network partition causes multiple nodes to believe they are the leader,
      leading to data inconsistency or conflicting actions.

    signature:
      - "Distributed system with leader election"
      - "Network can partition between nodes"
      - "Both sides of partition can accept writes"
      - "Conflict resolution is undefined or untested"

    anti_patterns:
      - "Leader election without proper quorum"
      - "Network timeout too long (false liveness)"
      - "No partition testing"
      - "Manual failover procedures"

    indicators:
      - "Intermittent network issues between nodes"
      - "Leader election events in logs"
      - "Data inconsistency reports"

    historical_examples:
      - "GitHub 2018: MySQL split-brain during maintenance"
      - "MongoDB incidents: Write conflicts after partition healing"

    mitigation_hints:
      - "Proper quorum-based consensus"
      - "Regular partition testing"
      - "Clear conflict resolution policies"

  - id: "DS-002"
    name: "Consensus Deadlock"
    description: |
      Distributed consensus cannot be reached because not enough nodes
      are available, leaving system stuck and unable to progress.

    signature:
      - "Consensus requires majority quorum"
      - "Node failures can drop below quorum"
      - "No degraded operation mode"
      - "Recovery requires manual intervention"

    anti_patterns:
      - "Quorum set too high"
      - "All nodes in same failure domain"
      - "No automated recovery"
      - "Insufficient redundancy"

    indicators:
      - "Node health fluctuations"
      - "Increasing operation latency"
      - "Timeout errors in consensus operations"

    historical_examples:
      - "etcd/Kubernetes: Cluster stuck when losing majority"
      - "ZooKeeper: Quorum loss freezes operations"

    mitigation_hints:
      - "Spread nodes across failure domains"
      - "Automated node recovery"
      - "Read-only degraded mode"

  # ============================================================================
  # NETWORKING PATTERNS
  # ============================================================================

  - id: "DS-003"
    name: "Cascading Timeout Amplification"
    description: |
      Timeouts at one layer cause retries that create more load, causing
      more timeouts, in an amplifying spiral.

    signature:
      - "Multiple service layers with timeouts"
      - "Retries configured at each layer"
      - "Timeout at layer N causes multiple calls at layer N-1"
      - "No circuit breakers"

    anti_patterns:
      - "Same timeout at all layers"
      - "Aggressive retry without backoff"
      - "No retry budget tracking"
      - "No load shedding"

    indicators:
      - "Request volume spikes during slowdowns"
      - "Latency increases at multiple layers simultaneously"
      - "Retry rate increasing"

    historical_examples:
      - "Amazon 2015: Retry storm during partial outage"
      - "Common microservice failure pattern"

    mitigation_hints:
      - "Timeout hierarchy (outer > inner)"
      - "Exponential backoff with jitter"
      - "Circuit breakers at service boundaries"

  - id: "DS-004"
    name: "Thundering Herd Recovery"
    description: |
      When a failed service recovers, all waiting requests hit it simultaneously,
      causing immediate re-failure.

    signature:
      - "Service with queued/waiting clients"
      - "Recovery announced to all clients simultaneously"
      - "No gradual traffic ramp-up"
      - "Service capacity < queued demand"

    anti_patterns:
      - "Instant failover to all traffic"
      - "No connection limiting"
      - "Cache stampede scenarios"
      - "No gradual rollout"

    indicators:
      - "Recovery followed by immediate re-failure"
      - "CPU/memory spike on recovery"
      - "Connection flood on service restart"

    historical_examples:
      - "Reddit frequent: Cache expiry stampede"
      - "Database restart under load: Immediate overload"

    mitigation_hints:
      - "Gradual traffic ramp-up"
      - "Request coalescing"
      - "Staggered cache expiry"

  - id: "DS-005"
    name: "DNS Dependency Blindspot"
    description: |
      System depends on DNS resolution but treats it as infallible.
      DNS issues cause mysterious, widespread failures.

    signature:
      - "Services discovered via DNS"
      - "Short DNS TTLs for flexibility"
      - "No DNS caching in application"
      - "DNS infrastructure not monitored"

    anti_patterns:
      - "DNS as single point of failure"
      - "Not caching DNS results"
      - "External DNS dependency for internal services"
      - "No DNS fallback"

    indicators:
      - "Random connection failures across services"
      - "Failures correlated with DNS changes"
      - "Recovery by restarting applications"

    historical_examples:
      - "Dyn 2016 DDoS: Cascading DNS failure"
      - "Facebook 2021: BGP/DNS dependency caused total outage"

    mitigation_hints:
      - "DNS caching with reasonable TTL"
      - "Multiple DNS providers"
      - "Internal DNS independence"

  # ============================================================================
  # STATE MANAGEMENT PATTERNS
  # ============================================================================

  - id: "DS-006"
    name: "Eventual Inconsistency Surprise"
    description: |
      Eventual consistency model accepted without understanding implications.
      Business logic assumes consistency that doesn't exist.

    signature:
      - "Distributed data with eventual consistency"
      - "Business logic assumes read-after-write consistency"
      - "Race conditions in user-facing flows"
      - "'Sometimes it works, sometimes not'"

    anti_patterns:
      - "Using eventual consistency for banking/inventory"
      - "Not communicating consistency model to consumers"
      - "Mixing consistency requirements in same system"
      - "No conflict resolution strategy"

    indicators:
      - "Intermittent data inconsistency reports"
      - "Race-condition related bugs"
      - "User complaints about 'missing' data"

    historical_examples:
      - "E-commerce: Double-selling inventory"
      - "Social media: Posts appearing/disappearing"

    mitigation_hints:
      - "Strong consistency where required"
      - "Explicit consistency model documentation"
      - "Conflict resolution automation"

  - id: "DS-007"
    name: "Distributed Transaction Phantom"
    description: |
      Multi-service transaction partially commits, leaving system in
      inconsistent state. Neither fully committed nor fully rolled back.

    signature:
      - "Operations spanning multiple services/databases"
      - "No distributed transaction coordinator"
      - "Partial failure leaves partial state"
      - "Manual cleanup required"

    anti_patterns:
      - "Assuming service calls are atomic"
      - "No compensation logic"
      - "No idempotency"
      - "Fire-and-forget for important operations"

    indicators:
      - "Orphaned records in database"
      - "Inconsistent state across services"
      - "Manual reconciliation needed"

    historical_examples:
      - "Payment + inventory: Money taken, item not reserved"
      - "Common saga pattern failures"

    mitigation_hints:
      - "Saga pattern with compensation"
      - "Idempotent operations"
      - "Transactional outbox"

  # ============================================================================
  # SCALING PATTERNS
  # ============================================================================

  - id: "DS-008"
    name: "Hot Shard Meltdown"
    description: |
      One partition/shard receives disproportionate traffic, overwhelming
      it while other shards are underutilized.

    signature:
      - "Sharded/partitioned data"
      - "Non-uniform access patterns"
      - "One shard hitting limits while others idle"
      - "Scaling out doesn't help"

    anti_patterns:
      - "Poor partition key choice"
      - "Celebrity/viral content problem"
      - "Time-based sharding with current-time bias"
      - "No hot shard detection"

    indicators:
      - "Uneven shard metrics"
      - "One shard consistently slower"
      - "Scaling doesn't improve performance"

    historical_examples:
      - "Twitter: Celebrity tweets hitting single shard"
      - "DynamoDB: Poor partition key causing throttling"

    mitigation_hints:
      - "Scatter-gather for hot keys"
      - "Adaptive sharding"
      - "Hot key caching"

  - id: "DS-009"
    name: "Autoscaling Oscillation"
    description: |
      Autoscaling adds capacity, load drops, capacity scales down,
      load returns, in unstable oscillation.

    signature:
      - "Aggressive autoscaling policies"
      - "Capacity changes affect measured metrics"
      - "Scale-up/scale-down ping-pong"
      - "Unstable resource allocation"

    anti_patterns:
      - "Scaling on lagging metrics"
      - "Symmetric up/down thresholds"
      - "No cooldown periods"
      - "Scaling on symptoms not causes"

    indicators:
      - "Frequent scale-up/scale-down events"
      - "Resource usage saw-tooth pattern"
      - "Scaling events correlated with issues"

    historical_examples:
      - "Common cloud scaling: Thrashing between states"
      - "Kubernetes HPA oscillation"

    mitigation_hints:
      - "Asymmetric scaling thresholds"
      - "Cooldown periods"
      - "Predictive scaling"

  # ============================================================================
  # RESILIENCE PATTERNS
  # ============================================================================

  - id: "DS-010"
    name: "Circuit Breaker Miscalibration"
    description: |
      Circuit breakers either trip too easily (causing unnecessary outages)
      or too slowly (not protecting against failures).

    signature:
      - "Circuit breakers implemented"
      - "Thresholds set arbitrarily or never tuned"
      - "Either never trips or always trips"
      - "No monitoring of circuit breaker state"

    anti_patterns:
      - "Copy-paste circuit breaker config"
      - "No testing of breaker behavior"
      - "Thresholds not based on actual failure patterns"
      - "No half-open state handling"

    indicators:
      - "Circuit breakers never trip (too loose)"
      - "Excessive breaker trips (too tight)"
      - "Breaker state not in dashboards"

    historical_examples:
      - "Common: Breakers either useless or over-sensitive"

    mitigation_hints:
      - "Baseline normal error rates"
      - "Test breaker behavior under failure"
      - "Monitor and alert on breaker state"

  - id: "DS-011"
    name: "Retry Storm Amplification"
    description: |
      Client retries during outage multiply load on recovering service,
      preventing recovery.

    signature:
      - "Clients retry failed requests"
      - "Retries immediate or with constant delay"
      - "Multiple client types retrying same service"
      - "No coordination of retry behavior"

    anti_patterns:
      - "Retry without exponential backoff"
      - "No jitter in retry delays"
      - "Unlimited retry attempts"
      - "No server-side pushback signal"

    indicators:
      - "Request rate multiplies during slowdown"
      - "Recovery takes longer than expected"
      - "Successful recovery followed by re-failure"

    historical_examples:
      - "Slack 2020: Retry storms during outage"
      - "Common pattern in microservice failures"

    mitigation_hints:
      - "Exponential backoff with jitter"
      - "Retry budgets"
      - "Server-side retry-after headers"

  - id: "DS-012"
    name: "Cascading Failure Propagation"
    description: |
      Failure in one service causes dependent services to fail,
      which causes their dependents to fail, domino-style.

    signature:
      - "Multiple service dependencies"
      - "No bulkheads between services"
      - "Synchronous calls across services"
      - "No graceful degradation"

    anti_patterns:
      - "Services waiting synchronously on dependencies"
      - "No fallback behavior"
      - "Shared thread pools across calls"
      - "No isolation between dependencies"

    indicators:
      - "Multiple services failing simultaneously"
      - "Root cause in single service"
      - "Recovery requires restarting multiple services"

    historical_examples:
      - "AWS 2017: S3 outage cascaded to many services"
      - "Netflix: Pioneered bulkhead pattern after cascades"

    mitigation_hints:
      - "Bulkhead pattern"
      - "Async where possible"
      - "Graceful degradation"
      - "Timeout at every boundary"
